{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7337 Homework 4\n",
    "\n",
    "Author: Nathan Wall\n",
    "\n",
    "Date: 6/25/2019\n",
    "\n",
    "The notebook below works through the questions as part of the homework 5. This notebook utilizes articles from MIT Technology Review for the purpose of academic work.\n",
    "\n",
    "Notebook Sections:\n",
    "- [Data Preperation](#pre)\n",
    "- [Q1: POS Tagger 1](#q1)\n",
    "- [Q2: POS Tagger 2](#q2)\n",
    "- [Q3: POS Tagger Comparison](#q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import nltk\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "<a id='pre'></a>\n",
    "\n",
    "In this section we scrape a recent news article and clean up some of the text for use in the rest of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.technologyreview.com/s/613738/artificial-intelligence-sees-construction-site-accidents-before-they-happen/amp/\n"
     ]
    }
   ],
   "source": [
    "#read from the artificial intelligence setion of MIT technology review\n",
    "page = requests.get('https://www.technologyreview.com/artificial-intelligence') \n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#find the article links\n",
    "webLinks = soup.find_all('h3', {\"class\": \"grid-tz__title\"})\n",
    "\n",
    "#select the first list article in that list \n",
    "articleLink = 'https://www.technologyreview.com'+webLinks[0].find('a').get('href')+'amp/' \n",
    "#technology article text require 'amp'\n",
    "\n",
    "print(articleLink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure the interpretations below make sense we will only use the following link for the rest of this HW. Although it would be possible to run the scraper at anytime to pull the most recent article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleLink = 'https://www.technologyreview.com/s/613738/artificial-intelligence-sees-construction-site-accidents-before-they-happen/amp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A construction site is a dangerous place to work, with a fatal accident rate five times higher than '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now lets parse that article\n",
    "page = requests.get(articleLink)\n",
    "# parse with BFS\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "pTags = soup.find_all('p')\n",
    "\n",
    "pList = []\n",
    "for p in pTags:\n",
    "    pList.append(p.get_text())\n",
    "\n",
    "article = [' '.join(pList)][0]\n",
    "article[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we are able to pull in one of the latest articles on the MIT Research Review website, to begin testing out diffee POS Taggers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Run one of the part-of-speech (POS) taggers available in Python. \n",
    "<a id='q1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will begin with the default pre-trained NLTK tagger based on the Penn Tree-bank tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_pos(sent):\n",
    "    #use NLTK default POS \"Penn Tree-Bank\"\n",
    "    #:sent: sentence to be tagged\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    d = dict();  \n",
    "    d['pos'] = nltk.pos_tag(tokens)\n",
    "    d['length'] = len(tokens)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we find the longest sentence\n",
    "sent = nltk.sent_tokenize(article)\n",
    "sent_list = []\n",
    "for s in sent:\n",
    "    sent_list.append(nltk_pos(s))\n",
    "    \n",
    "sent_df = pd.DataFrame(sent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Find the longest sentence you can, longer than 10 words, that the POS tagger tags correctly. Show the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46</td>\n",
       "      <td>[(“, NNS), (Most, RBS), (companies, NNS), (don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41</td>\n",
       "      <td>[(It, PRP), (can, MD), (then, RB), (be, VB), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35</td>\n",
       "      <td>[(Mary, NNP), (Gray, NNP), (,, ,), (an, DT), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>[(Jit, NNP), (Kee, NNP), (Chin, NNP), (,, ,), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>[(Suffolk, NNP), (,, ,), (a, DT), (constructio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    length                                                pos\n",
       "16      46  [(“, NNS), (Most, RBS), (companies, NNS), (don...\n",
       "6       41  [(It, PRP), (can, MD), (then, RB), (be, VB), (...\n",
       "20      35  [(Mary, NNP), (Gray, NNP), (,, ,), (an, DT), (...\n",
       "4       34  [(Jit, NNP), (Kee, NNP), (Chin, NNP), (,, ,), ...\n",
       "2       32  [(Suffolk, NNP), (,, ,), (a, DT), (constructio..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.sort_values('length', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest sentence in the article was pretty poorly tagged so I below are inputs and outputs from the second longest sentence from the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It can then be put to work monitoring a new construction site and flagging situations that seem likely to lead to an accident, such as worker not wearing gloves or working too close to a dangerous piece of machinery.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence has 41 words\n",
      "The tokens for each word are shown below\n",
      "('It', 'PRP')\n",
      "('can', 'MD')\n",
      "('then', 'RB')\n",
      "('be', 'VB')\n",
      "('put', 'VBN')\n",
      "('to', 'TO')\n",
      "('work', 'VB')\n",
      "('monitoring', 'VBG')\n",
      "('a', 'DT')\n",
      "('new', 'JJ')\n",
      "('construction', 'NN')\n",
      "('site', 'NN')\n",
      "('and', 'CC')\n",
      "('flagging', 'JJ')\n",
      "('situations', 'NNS')\n",
      "('that', 'WDT')\n",
      "('seem', 'VBP')\n",
      "('likely', 'JJ')\n",
      "('to', 'TO')\n",
      "('lead', 'VB')\n",
      "('to', 'TO')\n",
      "('an', 'DT')\n",
      "('accident', 'NN')\n",
      "(',', ',')\n",
      "('such', 'JJ')\n",
      "('as', 'IN')\n",
      "('worker', 'NN')\n",
      "('not', 'RB')\n",
      "('wearing', 'VBG')\n",
      "('gloves', 'NNS')\n",
      "('or', 'CC')\n",
      "('working', 'VBG')\n",
      "('too', 'RB')\n",
      "('close', 'RB')\n",
      "('to', 'TO')\n",
      "('a', 'DT')\n",
      "('dangerous', 'JJ')\n",
      "('piece', 'NN')\n",
      "('of', 'IN')\n",
      "('machinery', 'NN')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "#apply the tokenizer and display results\n",
    "print(\"The longest sentence has {} words\".format(sent_df['length'][6]))\n",
    "print(\"The tokens for each word are shown below\")\n",
    "for t in sent_df['pos'][6]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK POS tagger (Penn-Treebank) seems to get most of the 41 words from this sentence correctly tagged. All he common punctuation, determiners (DT), conjunctions (CC), & preposition (IN) are all flagged correctly. \n",
    "\n",
    "All of the nouns seem to appropriately tagged based whether they are plural (NNS) or singular (NN). \n",
    "\n",
    "Most of the adjectives (JJ) seem to be tagged correctly with the one potential exception of the word \"flagging\". This is tagged as and adjective but may be used a verb in this sentence. The rest of the verbs appear to be flagged correctly.\n",
    "\n",
    "Overall the POS tagging seems good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Find the shortest sentence you can, shorter than 10 words, that the POS tagger fails to tag 100 percent correctly. Show the input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>[(MIT, NNP), (Technology, NNP), (Review, NNP),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>[(Improving, VBG), (safety, NN), (is, VBZ), (a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>[(“, JJ), (Safety, NNP), (was, VBD), (a, DT), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>[(Deep-learning, JJ), (algorithms, NN), (typic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>[(And, CC), (it, PRP), (is, VBZ), (unlikely, J...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    length                                                pos\n",
       "22       5  [(MIT, NNP), (Technology, NNP), (Review, NNP),...\n",
       "14       8  [(Improving, VBG), (safety, NN), (is, VBZ), (a...\n",
       "15      13  [(“, JJ), (Safety, NNP), (was, VBD), (a, DT), ...\n",
       "13      13  [(Deep-learning, JJ), (algorithms, NN), (typic...\n",
       "19      16  [(And, CC), (it, PRP), (is, VBZ), (unlikely, J..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df.sort_values('length').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest sentence in the article is just the the footer naming the the website source. The only other sentence less than 10 words is sentence 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Improving safety is an incentive as well.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest sentence has 8 words\n",
      "The tokens for each word are shown below\n",
      "('Improving', 'VBG')\n",
      "('safety', 'NN')\n",
      "('is', 'VBZ')\n",
      "('an', 'DT')\n",
      "('incentive', 'NN')\n",
      "('as', 'RB')\n",
      "('well', 'RB')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "#apply the tokenizer and display results\n",
    "print(\"The shortest sentence has {} words\".format(sent_df['length'][14]))\n",
    "print(\"The tokens for each word are shown below\")\n",
    "for t in sent_df['pos'][14]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tagging seems to perform pretty well with the exception of the tag applied to 'as'. It tags 'as' with 'RB' indicating it is an adverb. However, it appears to be used as a preposition the adverb 'well' which is tagged correctly.\n",
    "\n",
    "Aside from that the rest of the sentence appears to tagged correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Run a different POS tagger in Python.\n",
    "<a id='q2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next tagger we are going to use Spacy's core english model (small). This is a pre-trained model that includes POS tags for the universal tagset. It was trained using primarily web text including news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Does it produce the same or different output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It can then be put to work monitoring a new construction site and flagging situations that seem likely to lead to an accident, such as worker not wearing gloves or working too close to a dangerous piece of machinery.\n"
     ]
    }
   ],
   "source": [
    "long_sent = nlp(sent[6])\n",
    "print(long_sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It PRON\n",
      "can VERB\n",
      "then ADV\n",
      "be VERB\n",
      "put VERB\n",
      "to ADP\n",
      "work NOUN\n",
      "monitoring VERB\n",
      "a DET\n",
      "new ADJ\n",
      "construction NOUN\n",
      "site NOUN\n",
      "and CCONJ\n",
      "flagging ADJ\n",
      "situations NOUN\n",
      "that DET\n",
      "seem VERB\n",
      "likely ADJ\n",
      "to PART\n",
      "lead VERB\n",
      "to ADP\n",
      "an DET\n",
      "accident NOUN\n",
      ", PUNCT\n",
      "such ADJ\n",
      "as ADP\n",
      "worker NOUN\n",
      "not ADV\n",
      "wearing VERB\n",
      "gloves NOUN\n",
      "or CCONJ\n",
      "working VERB\n",
      "too ADV\n",
      "close ADV\n",
      "to ADP\n",
      "a DET\n",
      "dangerous ADJ\n",
      "piece NOUN\n",
      "of ADP\n",
      "machinery NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in long_sent:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Short Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving safety is an incentive as well.\n"
     ]
    }
   ],
   "source": [
    "short_sent = nlp(sent[14])\n",
    "print(short_sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improving VERB\n",
      "safety NOUN\n",
      "is VERB\n",
      "an DET\n",
      "incentive NOUN\n",
      "as ADV\n",
      "well ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in short_sent:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Explain any differences as best you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output it seems that the two POS taggers produce roughly the same output with the biggest differences betweent the two being the tagsets. The Spacy Tagset is based on a \"universal\" tagset where the nltk defaults to the Penn-Treebank tagset.\n",
    "\n",
    "However, while the Penn-Treebank tags a much more in depth than the universal tagset the general groups the two taggers assigned the words too appears to be very similar. Even the same two tags that seemed questionable from NLTK tags are tagged the same way in the Spacy POS tags. \"Flagging\" from the longer sentence is still shown as an adjective and \"as\" from the shorter is still shown as an adverb.\n",
    "\n",
    "Overall, the only real difference is the difference in the tagset, but the results are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: In a news article from this week’s news, find a random sentence of at least 10 words.\n",
    "<a id='q3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And it is unlikely to stop there—we may all find ourselves working for algorithms eventually.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sent = random.choice(sent)\n",
    "random_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very interesting sentence, as was actually the first random sentence returned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Looking at the Penn tag set, manually POS tag the sentence yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'And': 'CC',\n",
       " 'it': 'PRP',\n",
       " 'is': 'VBZ',\n",
       " 'unlikeley': 'JJ',\n",
       " 'to': 'TO',\n",
       " 'stop': 'VB',\n",
       " 'there': 'RB',\n",
       " '-': '-',\n",
       " 'we': 'PRP',\n",
       " 'may': 'VB',\n",
       " 'all': 'JJ',\n",
       " 'find': 'VBD',\n",
       " 'ourselves': 'NNS',\n",
       " 'working': 'VBG',\n",
       " 'for': 'IN',\n",
       " 'algorithms': 'NNS',\n",
       " 'eventually': 'RB',\n",
       " '.': '.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = nltk.word_tokenize(random_sent)\n",
    "tokens = ['And', 'it', 'is', 'unlikeley', 'to', 'stop', 'there', '-', 'we', 'may', 'all', 'find', 'ourselves', 'working', 'for', 'algorithms', 'eventually','.']\n",
    "man_tags = ['CC','PRP', 'VBZ', 'JJ', 'TO', 'VB', 'RB', '-','PRP', 'VB', 'JJ', 'VBD', 'NNS', 'VBG', 'IN', 'NNS', 'RB','.']\n",
    "hand_tag = dict(zip(tokens,man_tags))\n",
    "hand_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Now run the same sentences through both taggers that you implemented for questions 1 and 2. Did either of the taggers produce the same results as you had created manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('And', 'CC')\n",
      "('it', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('unlikeley', 'JJ')\n",
      "('to', 'TO')\n",
      "('stop', 'VB')\n",
      "('there', 'EX')\n",
      "('-', ':')\n",
      "('we', 'PRP')\n",
      "('may', 'MD')\n",
      "('all', 'DT')\n",
      "('find', 'VB')\n",
      "('ourselves', 'NNS')\n",
      "('working', 'VBG')\n",
      "('for', 'IN')\n",
      "('algorithms', 'JJ')\n",
      "('eventually', 'RB')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "nltk_tag = nltk.pos_tag(tokens)\n",
    "for t in nltk_tag:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And CCONJ\n",
      "it PRON\n",
      "is VERB\n",
      "unlikely ADJ\n",
      "to PART\n",
      "stop VERB\n",
      "there ADV\n",
      "— PUNCT\n",
      "we PRON\n",
      "may VERB\n",
      "all ADV\n",
      "find VERB\n",
      "ourselves PRON\n",
      "working VERB\n",
      "for ADP\n",
      "algorithms NOUN\n",
      "eventually ADV\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "spacy_tag = nlp(random_sent)\n",
    "for t in spacy_tag:\n",
    "    print(t.text, t.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like all three methods provided different results which explore as part c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Explain any differences between the two taggers and your manual tagging as much as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three of the taggers seem to in line with each other until 'there'. Both my manual tagging and the Spacy tagging labeled it as an adverb. However, the NLTK tagging labeled it as an 'existential' there. I did not tag it that way in the manual as I missed that as an option, and the Spacy tag is using the universal tagset so did not have that option available.\n",
    "\n",
    "Again at the word 'may' my tags and Spacy tags disagree with the NLTK tags with the NLTK tagging it as a modal and the other two as a verb. Modal is a type of verb so these are pretty close.\n",
    "\n",
    "All there methods disagree on the work 'all'. This seems like a difficult word to correctly tag as I tagged it as an adjective, NLTK tagged it as a determinant, and Spacy tagged it as an adverb.\n",
    "\n",
    "The last term with interesting disagreement is 'algorithms'. While Spacy & myself tagged it as a noun, the NTLK tagger tagged it as an adjective. This seems like a rather simple task, which makes me wonder the age of the corpus used for the pre-trained NLTK tagger, and if that term is as prominently used as it is today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
