{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 7337 HW-8 Sentiment Analysis\n",
    "\n",
    "Author: Nathan Wall\n",
    "\n",
    "Date: 8/13/2019\n",
    "\n",
    "This notebook contains the code and visualizations for visualization of sample of reviews from IMDB from the crime genre\n",
    "\n",
    "Notebook Sections:\n",
    "- [Data Preperation](#prep)\n",
    "- [Clustering](#clustering)\n",
    "- [Sentiment](#sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/newall/anaconda3/envs/nlp/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "random.seed(13)\n",
    "import operator\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#text pre-processing\n",
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the reviews\n",
    "<a id='prep'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('msds7337_nwall_reviews.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "reviews = json.loads(data)\n",
    "corpus = [r['reviewText'] for r in reviews]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nice easy breezy murder mystery fun count deep sit popcorn soda enjoy movie offencive adult murder mystery romp like anymore ignore people like criticize think actual critic chemistry awesome hope movie\n"
     ]
    }
   ],
   "source": [
    "#load default pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess_review(text, remove_ne = True):\n",
    "    \"\"\" This function takes a document and applies some of preprocessing step\n",
    "\n",
    "    :param text: A string, the single document you want to process.\n",
    "    :param remove_ne: A boolean , whether you remove named entities from the text\n",
    "    \n",
    "    returns: A string with all tokens lemmatized & lower case with stop words, pronouns, and punctuation removed\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    if remove_ne == True:\n",
    "        named_entity = [i.text for i in doc.ents] #create list of named entities\n",
    "        tokens = [token.lemma_.lower() for token in doc #take the lower case lemmatized word\n",
    "                  if token.lemma_ != \"-PRON-\" #remove pronouns\n",
    "                  and token.pos_ != \"PROPN\" #remove propernouns\n",
    "                  and token.is_punct == False #remove punctiuation\n",
    "                  and token.is_stop == False #remove stopwords\n",
    "                  and token.is_digit == False #remove numbers\n",
    "                  and token.is_space == False #remove space\n",
    "                  and token.text not in named_entity #remove named entities\n",
    "                 ]\n",
    "    else:\n",
    "        tokens = [token.lemma_.lower() for token in doc #take the lower case lemmatized word\n",
    "                  if token.lemma_ != \"-PRON-\" #remove pronouns\n",
    "                  and token.is_punct == False #remove punctiuation\n",
    "                  and token.is_stop == False #remove stopwords\n",
    "                  and token.is_digit == False #remove numbers\n",
    "                  and token.is_space == False #remove space\n",
    "                 ]\n",
    "    tokens = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "tokens = preprocess_review(corpus[1])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219\n",
      "nice easy breezy murder mystery fun count deep sit popcorn soda enjoy movie offencive adult murder mystery romp like anymore ignore people like criticize think actual critic chemistry awesome hope movie\n"
     ]
    }
   ],
   "source": [
    "docs = [preprocess_review(doc) for doc in corpus]\n",
    "print(len(docs))\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering the reviews\n",
    "<a id='clustering'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 7196)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.6, min_df=1, stop_words='english', use_idf=True)\n",
    "tf_idf = vectorizer.fit_transform(docs)\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 372]\n",
      " [  1 552]\n",
      " [  2 144]\n",
      " [  3 151]]\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 4\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, init='k-means++')\n",
    "\n",
    "km4 = kmeans_model.fit(tf_idf)\n",
    "\n",
    "unique, counts = np.unique(km4.labels_, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "<a id='sentiment'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. In Python, load one of the sentiment vocabularies referenced in the textbook, and run the sentiment analyzer as explained in the corresponding reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/newall/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(docs[1])\n",
    "#remove the compound label\n",
    "del scores['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.282, 'neu': 0.306, 'pos': 0.412}\n",
      "The sentiment of this review is pos\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(\"The sentiment of this review is {}\".format(max(scores.items(), key=operator.itemgetter(1))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Original Sample Doc appears to be mostly positive which makes sense based on the content of that review shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. For each of the clusters you created in homework 7, compute the average, median, high, and low sentiment scores for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute sentiment for each review and build a dataframe.\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "scores = analyzer.polarity_scores(docs[0])\n",
    "df_sentiment = pd.DataFrame([scores], columns=scores.keys())\n",
    "\n",
    "for d in docs[1:]:\n",
    "    scores = analyzer.polarity_scores(d)\n",
    "    df_sentiment = df_sentiment.append(scores, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.141</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.7650</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.9244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.282</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.299</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound  cluster\n",
       "0  0.141  0.703  0.157    0.0772        2\n",
       "1  0.282  0.306  0.412    0.7650        3\n",
       "2  0.000  0.527  0.473    0.9244        0\n",
       "3  0.282  0.423  0.296    0.0258        2\n",
       "4  0.299  0.388  0.313    0.7783        1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentiment['cluster'] = km4.labels_\n",
    "df_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the sentiment for all the clustered data. Lets visulaize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summ = df_sentiment.groupby('cluster').agg({'neg':['mean', 'median', 'min', 'max'],\n",
    "                                     'pos':['mean', 'median', 'min', 'max'],\n",
    "                                     'compound':['mean', 'median', 'min', 'max']\n",
    "                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">neg</th>\n",
       "      <th colspan=\"4\" halign=\"left\">pos</th>\n",
       "      <th colspan=\"4\" halign=\"left\">compound</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.181387</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.311046</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.211208</td>\n",
       "      <td>0.22630</td>\n",
       "      <td>-0.9934</td>\n",
       "      <td>0.9947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.131467</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.350803</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.660251</td>\n",
       "      <td>0.94195</td>\n",
       "      <td>-0.9978</td>\n",
       "      <td>0.9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150208</td>\n",
       "      <td>0.1460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.311368</td>\n",
       "      <td>0.2855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.401949</td>\n",
       "      <td>0.65415</td>\n",
       "      <td>-0.9648</td>\n",
       "      <td>0.9976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151887</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.343132</td>\n",
       "      <td>0.3340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.537387</td>\n",
       "      <td>0.87200</td>\n",
       "      <td>-0.9860</td>\n",
       "      <td>0.9991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              neg                           pos                      compound  \\\n",
       "             mean  median  min    max      mean  median  min    max      mean   \n",
       "cluster                                                                         \n",
       "0        0.181387  0.0605  0.0  1.000  0.311046  0.3015  0.0  1.000  0.211208   \n",
       "1        0.131467  0.1170  0.0  0.778  0.350803  0.3415  0.0  1.000  0.660251   \n",
       "2        0.150208  0.1460  0.0  0.605  0.311368  0.2855  0.0  0.802  0.401949   \n",
       "3        0.151887  0.1330  0.0  0.646  0.343132  0.3340  0.0  0.767  0.537387   \n",
       "\n",
       "                                  \n",
       "          median     min     max  \n",
       "cluster                           \n",
       "0        0.22630 -0.9934  0.9947  \n",
       "1        0.94195 -0.9978  0.9990  \n",
       "2        0.65415 -0.9648  0.9976  \n",
       "3        0.87200 -0.9860  0.9991  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_summ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these reviews are largely positive in nature. Based off the findings in HW7 we know our reviews are more biased towards more popular & well liked shows/movies as these reviews come from the top50 Crime Movies/TV Shows on IMBD.\n",
    "\n",
    "It seems like the most neutral cluster is cluster 0, which is also our second biggest. This could our most critical reviews and probably a group of interest for the types of movie and TV shows in this group.\n",
    "\n",
    "Cluster 1 seems to be the most positive and also our largest cluster. Based on HW7 our largest cluster was usually a pretty noisy group but perhaps brought together by the 'positivity' of the language used.\n",
    "\n",
    "Cluster 2 & 3 are similar in both sentiment & size. It is interesting that neither have any entirely positive reviews like the other two. These two clusters are both very specific clusters so a common mostly positive sentiment is a little bit more informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
